import numpy as np # Linear algebra.
import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv).
import re
from nltk.corpus import stopwords # Provides list of words to remove from emails.
from nltk.stem.wordnet import WordNetLemmatizer # Words to neutral from (nouns by default) eg kills & killing > kill.
import string
import gensim # Topic modelling, document indexing and similarity retrieval.
from random import randint
import os
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix
from sklearn.svm import LinearSVC
import sys


def main():

    # Calls the "extract_csv_data" function to extract email data.
    print("\n>extracting email data..\n")
    emails = extract_csv_data(
        'emails-large.csv',
        ['file'],
        [['notes_inbox', 'discussion_threads']]
    )

    # Sends the email bodies to the "remove_duplicates" function.
    email_bodies = emails.message.as_matrix()
    print("\n>removing duplicates..\n")
    unique_emails = remove_duplicates(email_bodies)

    # Random number used to index a sample email later.
    random_email = randint(1, 5)

    # Print number of unique emails, followed by the randint email as a sample.
    print('There are a total of {} non-duplicate emails.\n'.format(len(unique_emails)))
    print('Sample email, unstructured content:\n\n', unique_emails[random_email])

    # Set the first 200000 emails as training set, the rest as testing set.
    # Send these emails to the "clean_data" function. This is a long process.
    print("\n>removing unecessary words..\n")
    training_set = clean_data(unique_emails[0:1000])
    testing_set = clean_data(unique_emails[1001:2000]) #currently unused

    # Print the randint email in the training set after being normalized.
    print('Sample email, normalized content:\n\n', training_set[random_email])

    process_dataset(training_set, testing_set)


def process_dataset(training_set, testing_set):

    # Implements the concept of Dictionary â€“ a mapping between words and their integer ids, using the training set.
    print("\n>generating dictionary..\n")
    dictionary = gensim.corpora.Dictionary(training_set)

    # Keep tokens that are contained in at least 20 documents (absolute number).
    # Keep tokens that are contained in no more than 0.1 documents (fraction of total corpus size, not absolute).
    # 0.1 being 10% of the Corpus, a word occurs in more than 10% it will be too common a term to warrant classifying.
    print("\n>filtering dictionary..\n")
    dictionary.filter_extremes(no_below=20, no_above=0.1)

    # Converts each doc into the bag-of-words format, list of (token_id, token_count).
    print("\n>generating training matrix..\n")
    matrix = [dictionary.doc2bow(doc) for doc in training_set]

    # TFIDF is a numerical statistic that aims to reflect the importance of a word within a document.
    # Output our tfidf_model: "TfidfModel(num_docs=200000, num_nnz=16017590)" where num_nnz is number of non-zero elements.
    tfidf_model = gensim.models.TfidfModel(matrix, id2word=dictionary)

    # LSI aims to identify patterns in relationships between terms and concepts in unstructured text.
    # Output our lsi_model: "LsiModel(num_terms=47483, num_topics=100, decay=1.0, chunksize=20000)
    lsi_model = gensim.models.LsiModel(tfidf_model[matrix], id2word=dictionary, num_topics=100)

    # Generate 100 topics containing 10 words each.
    topics = lsi_model.print_topics(num_topics=100, num_words=10)

    # Print out each topic generated by the lsi_model.
    for topic in topics:

        print(topic)

    # Below is repeating lines 107 - 125, but applied to the testing dataset of emails.
    # Please note that the dictionary used here has been previously generated from the training set.
    # This code will be cleaned eventually.
    print("\n>generating testing matrix..\n")
    matrix = [dictionary.doc2bow(doc) for doc in testing_set]
    tfidf_model = gensim.models.TfidfModel(matrix, id2word=dictionary)
    lsi_model = gensim.models.LsiModel(tfidf_model[matrix], id2word=dictionary, num_topics=100)
    topics = lsi_model.print_topics(num_topics=100, num_words=10)

    for topic in topics:

        print(topic)


# Explain this.
def extract_csv_data(file, cols_to_clean = [], exclude = [[]]):

    data = pd.read_csv(file)
    print("Email info Below:")
    print(data['message'][20])

    for i, col in enumerate(cols_to_clean):
        exclude_pattern = re.compile('|'.join(exclude[i]))
        data = data[data[col].str.contains(exclude_pattern) == False]

    print(len(data), "is the length of the data.")

    return data


# Compares the emails in the CSV and removes duplicate emails.
def remove_duplicates(data):

    processed = set()
    result = []
    pattern = re.compile('X-FileName: .*')
    pattern2 = re.compile('X-FileName: .*?  ')

    for doc in data:
        doc = doc.replace('\n', ' ')
        doc = doc.replace(' .*?nsf', '')
        match = pattern.search(doc).group(0)
        match = re.sub(pattern2, '', match)

        if match not in processed:
            processed.add(match)
            result.append(match)

    return result


# Sends each doc to the "clean" function.
def clean_data(data):

    return [clean(doc).split(' ') for doc in data]


# Clean up the data, remove unwanteds words that are such as I, A etc.. (stopwords.word('english')).
def clean(doc):

    words_to_exclude = set(stopwords.words('english'))
    exclude = set(string.punctuation)
    lemma = WordNetLemmatizer()

    word_free = " ".join([i for i in doc.lower().split() if i not in words_to_exclude])
    punc_free = ''.join(ch for ch in word_free if ch not in exclude)
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())

    return normalized


main()